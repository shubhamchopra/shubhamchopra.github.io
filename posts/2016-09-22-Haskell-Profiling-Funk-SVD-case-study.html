<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>Shubham Chopra - Haskell Profiling and Performance Optimization - Funk SVD</title>
        <link rel="stylesheet" type="text/css" href="../css/brooklyn.css" />
        <link rel="stylesheet" type="text/css" href="../css/syntax.css" />
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <div id="header">
            <div id="logo">
                <a href="../">Shubham Chopra</a>
            </div>
            <div id="navigation">
                <a href="../">Home</a>
                <a href="../about.html">About</a>
                <a href="../contact.html">Contact</a>
                <a href="../archive.html">Archive</a>
            </div>
        </div>

        <div id="content">
            <h1>Haskell Profiling and Performance Optimization - Funk SVD</h1>

            <div class="info">
    Posted on September 22, 2016
    
</div>
<section>
<h2 id="recommendation-systems-a-brief-and-definitely-not-comprehensive-introduction">Recommendation Systems: A brief and definitely not comprehensive introduction</h2>
<p>Recommendation systems today are ubiquitous. Almost all websites use them for everything from recommending content to read to movies you might like to watch to products you might want to purchase. When there are plenty of options and resources like time or money or both are scarce, these act like your friends to help guide you through a maze of options and come out reasonably satisfied from the experience. If you did not like the experience or even if you did like it, you will rate whatever it is you consumed and the system will <em>learn</em> from it and try to be better at it the next time. In essence, they solve a quintessentially <em>first world</em> problem of plenty.</p>
<p>It is this <em>learning</em> that is an interesting problem. Large amounts of data is collected on what different users consumed, and how they rated it, and is used to come up with mathematical models that can predict, based on the past experiences, what the users might like. At the core, therefore, is a user-item matrix that contains ratings of various items by various users. This matrix, almost always, is extremely sparse. I bet you don’t know anyone who has seen all the movies on Netflix, for example. (If you do, that person probably needs a recommendation system to tell them to go outside once in a while, the graphics are really amazing!) Most people would have only consumed a very small subset of the entire universe of product a business deals in.</p>
<p>While recommender systems have existed for some time now, the problem itself got it’s biggest marketing boost courtesy the <a href="http://www.netflixprize.com/"><em>Netflix Challenge</em></a>. Netflix gave a sizeable amount of data to train the models and that attracted a lot of researchers. Not to mention the million dollars. One such researcher goes by a pseudonym “Simon Funk” and came up with a technique that is now called <a href="http://sifter.org/~simon/journal/20061211.html"><em>Funk SVD</em></a>.</p>
<p>Very briefly, it is a matrix factorization technique (Singular Value Decomposition) to decompose the ratings matrix. As a result of this factorization, we end up with user vectors and item vectors, each describing some salient preferences of users and salient features of items. Once we get this, we can now use dot product of a user vector with an item vector to get an estimate of how a user would rate the item.</p>
<p>I will not pretend to be an expert in recommendation systems. There are several online courses that go in-depth into various techniques that can be used for recommendations. I personally completed and enjoyed the “Introduction to Recommender Systems” course on <a href="https://www.coursera.org">Coursera</a>, but I see that it has been made into a specialization <a href="https://www.coursera.org/specializations/recommender-systems">now</a>.</p>
<p>Since this post is about Haskell and profiling, lets now jump right into it. The rest of this post is arranged as follows: We start with a preliminary implementation of Funk SVD that gives us the required output. We then profile it using tools available in GHC. GHC has profilers that can help us understand how and where memory is being used. Based on what we glean from the profiling tools, we try to modify code to minimize memory usage.</p>
<h2 id="funk-svd---haskell-implementation">Funk SVD - Haskell implementation</h2>
<p>We will go over a brief code overview to understand the flow. The src/Main.hs file contains the main function, no surprises there. I read a gzip compressed rating file, that contains ratings in the form “rating|user|item” where the rating is a double, user and items are both integers. We read this file each entry is used to create a <em>Rating</em> object. We also create the <em>Config</em> object that fills up some parameters used in estimation.</p>
<p>The meat of the code is in <a href="https://github.com/shubhamchopra/RecommenderSystem/blob/master/InitialVersion/src/FunkSVD/SerialEstimator.hs">SerialEstimator.hs</a>. The following three functions are central to the logic:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">runIteration ::</span> <span class="dt">Config</span> 
  <span class="ot">-&gt;</span> (<span class="dt">M.Map</span> <span class="dt">User</span> <span class="dt">Double</span>, <span class="dt">M.Map</span> <span class="dt">Item</span> <span class="dt">Double</span>) 
  <span class="ot">-&gt;</span> <span class="dt">DV.Vector</span> <span class="dt">Rating</span> 
  <span class="ot">-&gt;</span> (<span class="dt">M.Map</span> <span class="dt">User</span> <span class="dt">Double</span>, <span class="dt">M.Map</span> <span class="dt">Item</span> <span class="dt">Double</span>)
runIteration config <span class="fu">=</span> DV.foldl' f
  <span class="kw">where</span>
    f (uV, iV) (<span class="dt">Rating</span> r u i) <span class="fu">=</span> (uV', iV')
      <span class="kw">where</span>
        dStart <span class="fu">=</span> startingEstimate config
        lRate <span class="fu">=</span> learningRate config
        reg <span class="fu">=</span> regFactor config
        ua  <span class="fu">=</span> M.findWithDefault dStart u uV
        ia  <span class="fu">=</span> M.findWithDefault dStart i iV
        eps <span class="fu">=</span> r <span class="fu">-</span> ua <span class="fu">*</span> ia
        dua <span class="fu">=</span> lRate <span class="fu">*</span> (eps <span class="fu">*</span> ia <span class="fu">-</span> reg <span class="fu">*</span> ua)
        dia <span class="fu">=</span> lRate <span class="fu">*</span> (eps <span class="fu">*</span> ua <span class="fu">-</span> reg <span class="fu">*</span> ia)
        uV' <span class="fu">=</span> M.insert u (ua <span class="fu">+</span> dua) uV
        iV' <span class="fu">=</span> M.insert i (ia <span class="fu">+</span> dia) iV</code></pre></div>
<p>This function runs a single iteration of the estimation. It takes a vector of user and item dimension, along with the vector of ratings, and returns an updated vector of user and item dimensions. The update is as explained in the Funk SVD algorithm.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">runTillConvergenceIO ::</span> <span class="dt">Config</span> <span class="ot">-&gt;</span> <span class="dt">FunkSVD</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">FunkSVD</span>
runTillConvergenceIO config funkSVD <span class="fu">=</span>
  <span class="kw">do</span>
  stateMaps <span class="ot">&lt;-</span> newIORef (M.empty, M.empty)
  forM_ [<span class="dv">1</span> <span class="fu">..</span> (numIterations config)] <span class="fu">$</span> \_ <span class="ot">-&gt;</span> <span class="kw">do</span>
    maps <span class="ot">&lt;-</span> readIORef stateMaps
    shuffledResiduals <span class="ot">&lt;-</span> R.shuffleVectorIO <span class="fu">$</span> residuals funkSVD
    writeIORef stateMaps <span class="fu">$</span> runIteration config maps shuffledResiduals
  (dUv, dIv) <span class="ot">&lt;-</span> readIORef stateMaps
  <span class="kw">let</span> residuals' <span class="fu">=</span> updateResiduals (residuals funkSVD) dUv dIv
      uV <span class="fu">=</span> userVectors funkSVD
      iV <span class="fu">=</span> itemVectors funkSVD
  return funkSVD{residuals <span class="fu">=</span> residuals'
                 , userVectors <span class="fu">=</span> appendVectors uV dUv
                 , itemVectors <span class="fu">=</span> appendVectors iV dIv}</code></pre></div>
<p>This function takes the input, and runs a number of iterations to estimate a single user and item dimension. Funk SVD algorithm recommends that we randomize the ratings vector between the estimation iterations for better generalization.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">runFullEstimationIO ::</span> <span class="dt">DV.Vector</span> <span class="dt">Rating</span> <span class="ot">-&gt;</span> <span class="dt">Config</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">FunkSVD</span>
runFullEstimationIO ratings config <span class="fu">=</span>
  <span class="kw">do</span>
  (trainingSet, testSet) <span class="ot">&lt;-</span> R.randomSplitIO (<span class="dv">4</span> <span class="fu">*</span> DV.length ratings <span class="ot">`div`</span> <span class="dv">5</span>) ratings
  <span class="kw">let</span> funkSVDInput <span class="fu">=</span> runReader (generateFunkSVDInput trainingSet) config
  funkSVDRef <span class="ot">&lt;-</span> newIORef funkSVDInput
  forM_ [<span class="dv">1</span> <span class="fu">..</span> (numDimensions config)] <span class="fu">$</span> \dim <span class="ot">-&gt;</span> <span class="kw">do</span>
    putStrLn <span class="fu">$</span> <span class="st">&quot;Working on dimension &quot;</span> <span class="fu">++</span> show dim
    funkSVD <span class="ot">&lt;-</span> readIORef funkSVDRef
    funkSVD' <span class="ot">&lt;-</span> runTillConvergenceIO config funkSVD
    writeIORef funkSVDRef funkSVD'
    putStrLn <span class="fu">$</span> <span class="st">&quot;Average residuals = &quot;</span> <span class="fu">++</span> (show <span class="fu">.</span> globalMean abs) (residuals funkSVD')
    putStrLn <span class="fu">$</span> <span class="st">&quot;Residuals RMSE = &quot;</span> <span class="fu">++</span> show (sqrt (globalMean (<span class="fu">**</span><span class="dv">2</span>) (residuals funkSVD')))
    putStrLn <span class="fu">$</span> <span class="st">&quot;Training RMSE = &quot;</span> <span class="fu">++</span> show (getRMSE trainingSet funkSVD')
    putStrLn <span class="fu">$</span> <span class="st">&quot;Test RMSE = &quot;</span> <span class="fu">++</span> show (getRMSE testSet funkSVD')
  readIORef funkSVDRef</code></pre></div>
<p>This is the function that runs the top-level loop that tries to estimate all the dimensions. It starts by randomly keeping aside a 20% dataset to act as an out-of-sample test set. The remaining data is used for training. It also prints the mean and RMSE error along the way.</p>
<p>This should be enough for now, we will go into other functions as and when needed. Let’s jump right into profiling now.</p>
<h2 id="memory-profiling-in-ghc">Memory profiling in GHC</h2>
<p>GHC comes with a decent set of tools to get an estimate of memory usage. All the techniques we are using here are described in <a href="http://book.realworldhaskell.org/read/profiling-and-optimization.html">Real World Haskell</a>. We start with the very basic: <em>+RTS -s</em>. This is run using the command <code>stack exec -- RecommendationSystemInitialVersionProfiling ./testRatings.gz +RTS -s</code>. This prints out GC and memory stats at the end of the run. I have used a significantly small input, about 10k ratings, to keep the runtimes small for faster iterations. Let’s take our initial version for a spin and see how it performs.</p>
<pre><code>  13,760,123,472 bytes allocated in the heap
  16,451,043,328 bytes copied during GC
     386,253,360 bytes maximum residency (126 sample(s))
      17,518,264 bytes maximum slop
             685 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0     26684 colls,     0 par    6.292s   6.316s     0.0002s    0.0010s
  Gen  1       126 colls,     0 par    8.131s   8.931s     0.0709s    0.3639s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.000s  (  0.001s elapsed)
  MUT     time    6.387s  (  8.333s elapsed)
  GC      time   12.505s  ( 13.328s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    1.918s  (  1.919s elapsed)
  EXIT    time    0.000s  (  0.001s elapsed)
  Total   time   20.843s  ( 21.663s elapsed)

  Alloc rate    2,154,403,840 bytes per MUT second

  Productivity  30.8% of total user, 29.6% of total elapsed
</code></pre>
<p>The <em>bytes maximum residency</em> is the maximum number of bytes our process was using. This number is only an estimate as it is only measured during a major GC when older generations (Gen 1 in this case) is also garbage collected. The other number, <em>685 MB total memory in use</em> on the other hand tells us whats the maximum amount of memory this process had taken from the OS. This is the number to focus on, and is pretty huge for the size of the input we have here. It also gives us the break-down of what time was spent on the process (MUT) vs GC. Finally, productivity tells us that only 31% of the time was spent doing our number crunching. The rest was spent garbage collecting. This code is screaming for some optimization.</p>
<p>So let’s try to find out the memory hogs in our code. We do this by building the binaries with profiling enabled. We then execute the binaries using a command like <code>stack exec -- RecommendationSystemInitialVersionProfiling ./testRatings.gz +RTS -s -p -hc</code>. This creates a profiling output file <code>RecommendationSystemInitialVersionProfiling.hp</code>. This information can be converted to a graphic using <code>hp2ps -e8in -c &lt;filename.hp&gt;</code>. This creates a PS file with a graph that shows memory usage over time. The <em>-hc</em> option creates a graph that shows memory usage by cost center stack which produced the data, so mostly the functions and closures that might be producing the data.</p>
<div class="figure">
<img src="../images/2016-09-22-HaskellProfiling-InitialVersion-hc-v1.jpg" title="Initial version, memory usage using -hc" alt="Initial version, memory usage using -hc" />
<p class="caption">Initial version, memory usage using -hc</p>
</div>
<p>That doesn’t leave any doubts about which function is the memory hog here. Lets see what types are actually taking up all that space. This can be done by running the command <code>stack exec -- RecommendationSystemInitialVersionProfiling ./testRatings.gz +RTS -s -p -hy</code>.</p>
<div class="figure">
<img src="../images/2016-09-22-HaskellProfiling-InitialVersion-hy-v1.jpg" title="Initial version, memory usage using -hy" alt="Initial version, memory usage using -hy" />
<p class="caption">Initial version, memory usage using -hy</p>
</div>
<p>This makes it clear that Maps and Doubles are taking up enormous amounts of space. Something fishy is going on in the <code>runIteration</code> function. We get a clue from the fact that the memory usage spikes up and then suddenly goes down. This cycles goes on for 5 times, which is the same as the number of dimensions we are running. Now, at the end of the run for each dimension, we print some stats about errors and such. This is what is causing the release of all the allocated memory. Looking at the code, that specific function is all about reading from a map, doing some evaluations on doubles and inserting them back in the map in a accumulating foldl loop. It is pretty clear that the maps aren’t really getting evaluated at every step, but are being built up on in thunks in true Haskell lazy fashion. How can we enforce these maps to be evaluated at each step? We use <em>BangPatterns</em>! We also use the <code>seq</code> operator. Let’s change the function to the one below and see the effect it has on memory.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">runIteration ::</span> <span class="dt">Config</span> 
  <span class="ot">-&gt;</span> (<span class="dt">M.Map</span> <span class="dt">User</span> <span class="dt">Double</span>, <span class="dt">M.Map</span> <span class="dt">Item</span> <span class="dt">Double</span>) 
  <span class="ot">-&gt;</span> <span class="dt">DV.Vector</span> <span class="dt">Rating</span> 
  <span class="ot">-&gt;</span> (<span class="dt">M.Map</span> <span class="dt">User</span> <span class="dt">Double</span>, <span class="dt">M.Map</span> <span class="dt">Item</span> <span class="dt">Double</span>)
runIteration config <span class="fu">=</span> DV.foldl' f
  <span class="kw">where</span>
    f (<span class="fu">!</span>uV, <span class="fu">!</span>iV) (<span class="dt">Rating</span> r u i) <span class="fu">=</span> (uV', iV')
      <span class="kw">where</span>
        dStart <span class="fu">=</span> startingEstimate config
        lRate <span class="fu">=</span> learningRate config
        reg <span class="fu">=</span> regFactor config
        ua  <span class="fu">=</span> M.findWithDefault dStart u uV
        ia  <span class="fu">=</span> M.findWithDefault dStart i iV
        eps <span class="fu">=</span> r <span class="fu">-</span> ua <span class="fu">*</span> ia
        dua <span class="fu">=</span> lRate <span class="fu">*</span> (eps <span class="fu">*</span> ia <span class="fu">-</span> reg <span class="fu">*</span> ua)
        dia <span class="fu">=</span> lRate <span class="fu">*</span> (eps <span class="fu">*</span> ua <span class="fu">-</span> reg <span class="fu">*</span> ia)
        ua' <span class="fu">=</span> ua <span class="fu">+</span> dua
        ia' <span class="fu">=</span> ia <span class="fu">+</span> dua
        uV' <span class="fu">=</span> ua' <span class="ot">`seq`</span> M.insert u ua' uV
        iV' <span class="fu">=</span> ia' <span class="ot">`seq`</span> M.insert i ia' iV</code></pre></div>
<p>Running it through the same set of commands, we see the following memory usage profile:</p>
<pre><code>  12,760,008,856 bytes allocated in the heap
   3,710,445,272 bytes copied during GC
      54,785,880 bytes maximum residency (91 sample(s))
       2,281,096 bytes maximum slop
             107 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0     24886 colls,     0 par    1.578s   1.583s     0.0001s    0.0005s
  Gen  1        91 colls,     0 par    1.527s   1.653s     0.0182s    0.0509s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.001s  (  0.001s elapsed)
  MUT     time    5.916s  (  6.394s elapsed)
  GC      time    2.659s  (  2.791s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    0.446s  (  0.446s elapsed)
  EXIT    time    0.000s  (  0.001s elapsed)
  Total   time    9.054s  (  9.187s elapsed)

  Alloc rate    2,156,874,450 bytes per MUT second

  Productivity  65.7% of total user, 64.8% of total elapsed
</code></pre>
<p>Alright! We got the time down to a little over 9s. We got the productivity to 65% and we reduced the total memory usage by a factor of 6! That is some saving for a few innocuous looking changes. What exactly is going on there? With the <em>BangPatterns</em>, we are telling the compiler before-hand that we expect this parameter to be strictly evaluated since we know we would use all the information from it. It is syntactic sugar for the <code>seq</code> function that has the prototype <code>seq :: a -&gt; b -&gt; b</code>. This function evaluates the first argument to WHNF before returning the second argument. We are enforcing all the evaluations to complete before moving forward with the next iteration in the foldl.</p>
<p>Lets take a look at the graphs, as they would now tell us if we are done.</p>
<div class="figure">
<img src="../images/2016-09-22-HaskellProfiling-InitialVersion-hc-v2.jpg" title="memory usage using -hc" alt="memory usage using -hc" />
<p class="caption">memory usage using -hc</p>
</div>
<div class="figure">
<img src="../images/2016-09-22-HaskellProfiling-InitialVersion-hy-v2.jpg" title="memory usage using -hy" alt="memory usage using -hy" />
<p class="caption">memory usage using -hy</p>
</div>
<p>The peaks have gone down considerably, but we still see the same pattern of high allocation and then de-allocation. The first graph tells us that a big portion of this is in the <em>shuffleVectorST</em> code. Lets take a look at the code. This code was adapted from <a href="https://wiki.haskell.org/Random_shuffle">Random Shuffle</a>:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">shuffleVectorST ::</span> <span class="dt">R.RandomGen</span> t <span class="ot">=&gt;</span> <span class="dt">DV.Vector</span> a <span class="ot">-&gt;</span> t <span class="ot">-&gt;</span> (<span class="dt">DV.Vector</span> a, t)
shuffleVectorST xs gen <span class="fu">=</span> runST <span class="fu">$</span> <span class="kw">do</span>
  g <span class="ot">&lt;-</span> newSTRef gen
  <span class="kw">let</span> n <span class="fu">=</span> DV.length xs
      randomRST lohi <span class="fu">=</span> <span class="kw">do</span>
        (a, s') <span class="ot">&lt;-</span> liftM (R.randomR lohi) (readSTRef g)
        writeSTRef g s'
        return a
      copyVectorToMutable xs' <span class="fu">=</span> <span class="kw">do</span>
        v <span class="ot">&lt;-</span> GM.new n
        forM_ [<span class="dv">0</span> <span class="fu">..</span> (n <span class="fu">-</span> <span class="dv">1</span>)] <span class="fu">$</span> \i <span class="ot">-&gt;</span>
          GM.write v i (xs' <span class="fu">DV.!</span> i)
        return v

  v <span class="ot">&lt;-</span> copyVectorToMutable xs
  forM_ [<span class="dv">0</span> <span class="fu">..</span> (n <span class="fu">-</span> <span class="dv">1</span>)] <span class="fu">$</span> \i <span class="ot">-&gt;</span> <span class="kw">do</span>
    j <span class="ot">&lt;-</span> randomRST (i, n <span class="fu">-</span> <span class="dv">1</span>)
    GM.swap v i j
  ret <span class="ot">&lt;-</span> DV.unsafeFreeze v
  g' <span class="ot">&lt;-</span> readSTRef g
  return (ret, g')</code></pre></div>
<p>This implements Fisher-Yates shuffle with a mutable vector. The reason for this choice was that modifying a vector is <em>O(n)</em>. It can also be implemented by using Maps, but Maps in Haskell have complexity of <em>log(n)</em> for both finds and inserts. This would bring the shuffle complexity with Maps to <em>n log(n)</em>.</p>
<p>To shuffle a vector, we are taking care to copy it to a mutable vector so we do the shuffle non-destructively. But that is manifesting itself as the big spike in memory usage. Also note the <em>MUT_ARR_PTRS_FROZEN</em> part, which may not be significant compared to the peak, but shows the same pattern. That section is related to the GHC putting guard rails around anything mutable since that needs special care. Let’s see if we can modify the code. Instead of copying the vector, we can permute a vector of contiguous indices and then back-permute that to get a new vector. The code would look something like the following:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">shuffleVectorST ::</span> <span class="dt">R.RandomGen</span> t <span class="ot">=&gt;</span> <span class="dt">DV.Vector</span> a <span class="ot">-&gt;</span> t <span class="ot">-&gt;</span> (<span class="dt">DV.Vector</span> a, t)
shuffleVectorST <span class="fu">!</span>xs gen <span class="fu">=</span> runST <span class="fu">$</span> <span class="kw">do</span>
  g <span class="ot">&lt;-</span> newSTRef gen
  <span class="kw">let</span> n <span class="fu">=</span> DV.length xs
      randomRST lohi <span class="fu">=</span> <span class="kw">do</span>
        (a, s') <span class="ot">&lt;-</span> liftM (R.randomR lohi) (readSTRef g)
        writeSTRef g s'
        return a
      indexVector <span class="fu">=</span> DV.thaw <span class="fu">$</span> DV.generate n id

  v <span class="ot">&lt;-</span> indexVector
  forM_ [<span class="dv">0</span> <span class="fu">..</span> (n <span class="fu">-</span> <span class="dv">1</span>)] <span class="fu">$</span> \i <span class="ot">-&gt;</span> <span class="kw">do</span>
    j <span class="ot">&lt;-</span> randomRST (i, n <span class="fu">-</span> <span class="dv">1</span>)
    GM.swap v i j
  ret <span class="ot">&lt;-</span> DV.freeze v
  g' <span class="ot">&lt;-</span> readSTRef g
  return (DV.backpermute xs ret, g')</code></pre></div>
<p>Let’s check out the performance with this change.</p>
<pre><code>  12,463,617,336 bytes allocated in the heap
   2,334,594,160 bytes copied during GC
       9,553,624 bytes maximum residency (193 sample(s))
       1,260,264 bytes maximum slop
              21 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0     24094 colls,     0 par    1.424s   1.429s     0.0001s    0.0004s
  Gen  1       193 colls,     0 par    0.640s   0.652s     0.0034s    0.0150s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.000s  (  0.001s elapsed)
  MUT     time    5.722s  (  5.802s elapsed)
  GC      time    2.014s  (  2.030s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    0.051s  (  0.052s elapsed)
  EXIT    time    0.001s  (  0.001s elapsed)
  Total   time    7.817s  (  7.833s elapsed)

  Alloc rate    2,178,072,188 bytes per MUT second

  Productivity  73.6% of total user, 73.4% of total elapsed
</code></pre>
<p>So we just shaved off a little over a second from the total run-time. The productivity is up to about 74% now. We are also using a lot less memory. So we made some progress. Let’s look at the graphs now:</p>
<div class="figure">
<img src="../images/2016-09-22-HaskellProfiling-InitialVersion-hc-v3.jpg" title="memory usage using -hc" alt="memory usage using -hc" />
<p class="caption">memory usage using -hc</p>
</div>
<div class="figure">
<img src="../images/2016-09-22-HaskellProfiling-InitialVersion-hy-v3.jpg" title="memory usage using -hy" alt="memory usage using -hy" />
<p class="caption">memory usage using -hy</p>
</div>
<p>It looks like we got rid of the previous peaks, but the MUT_ARR_PTRS_FROZEN is now the prominant pattern, again doing a lot of allocation and de-allocation. Do we have any <em>mutations</em> happenning elsewhere in the code? The usage of IORef in <code>runFullEstimationIO</code> and <code>runTillConvergenceIO</code> seems suspect. In both the cases, we are accumulating state. Since we want to emit some status to stdout, we are stuck in the IO monad. There is one function that might help us here. That is <code>foldM :: Monad m =&gt; (a -&gt; b -&gt; m a) -&gt; a -&gt; [b] -&gt; m a</code>. Using this, we should be able to get rid of IORefs in the two functions. The code for these would now look like this:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">runTillConvergenceIO ::</span> <span class="dt">Config</span> <span class="ot">-&gt;</span> <span class="dt">FunkSVD</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">FunkSVD</span>
runTillConvergenceIO config funkSVD <span class="fu">=</span>
  <span class="kw">do</span>

  <span class="kw">let</span> func (<span class="fu">!</span>dUv, <span class="fu">!</span>dIv) _ <span class="fu">=</span> <span class="kw">do</span>
        shuffledResiduals <span class="ot">&lt;-</span> R.shuffleVectorIO <span class="fu">$</span> residuals funkSVD
        return <span class="fu">$</span> runIteration config (dUv, dIv) shuffledResiduals

  (dUv, dIv) <span class="ot">&lt;-</span> foldM func (M.empty, M.empty) [<span class="dv">1</span> <span class="fu">..</span> (numIterations config)]

  <span class="kw">let</span> residuals' <span class="fu">=</span> updateResiduals (residuals funkSVD) dUv dIv
      uV <span class="fu">=</span> userVectors funkSVD
      iV <span class="fu">=</span> itemVectors funkSVD
  return funkSVD{residuals <span class="fu">=</span> residuals', userVectors <span class="fu">=</span> appendVectors uV dUv, itemVectors <span class="fu">=</span> appendVectors iV dIv}

<span class="ot">runFullEstimationIO ::</span> <span class="dt">DV.Vector</span> <span class="dt">Rating</span> <span class="ot">-&gt;</span> <span class="dt">Config</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">FunkSVD</span>
runFullEstimationIO ratings config <span class="fu">=</span>
  <span class="kw">do</span>
  (trainingSet, testSet) <span class="ot">&lt;-</span> R.randomSplitIO (<span class="dv">4</span> <span class="fu">*</span> DV.length ratings <span class="ot">`div`</span> <span class="dv">5</span>) ratings
  <span class="kw">let</span> funkSVDInput <span class="fu">=</span> runReader (generateFunkSVDInput trainingSet) config

  <span class="kw">let</span> func <span class="fu">!</span>funkSVD dim <span class="fu">=</span> <span class="kw">do</span>
        putStrLn <span class="fu">$</span> <span class="st">&quot;Working on dimension &quot;</span> <span class="fu">++</span> show dim
        funkSVD' <span class="ot">&lt;-</span> runTillConvergenceIO config funkSVD
        putStrLn <span class="fu">$</span> <span class="st">&quot;Average residuals = &quot;</span> <span class="fu">++</span> (show <span class="fu">.</span> globalMean abs) (residuals funkSVD')
        putStrLn <span class="fu">$</span> <span class="st">&quot;Residuals RMSE = &quot;</span> <span class="fu">++</span> show (sqrt (globalMean (<span class="fu">**</span><span class="dv">2</span>) (residuals funkSVD')))
        putStrLn <span class="fu">$</span> <span class="st">&quot;Training RMSE = &quot;</span> <span class="fu">++</span> show (getRMSE trainingSet funkSVD')
        putStrLn <span class="fu">$</span> <span class="st">&quot;Test RMSE = &quot;</span> <span class="fu">++</span> show (getRMSE testSet funkSVD')
        return funkSVD'

  foldM func funkSVDInput [<span class="dv">1</span> <span class="fu">..</span> (numDimensions config)]</code></pre></div>
<p>With that done, lets take this for a spin.</p>
<pre><code>  10,760,815,200 bytes allocated in the heap
   2,400,955,744 bytes copied during GC
       4,096,808 bytes maximum residency (291 sample(s))
         336,232 bytes maximum slop
              12 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0     20627 colls,     0 par    1.261s   1.266s     0.0001s    0.0004s
  Gen  1       291 colls,     0 par    0.441s   0.443s     0.0015s    0.0027s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.000s  (  0.001s elapsed)
  MUT     time    5.404s  (  5.478s elapsed)
  GC      time    1.655s  (  1.661s elapsed)
  RP      time    0.000s  (  0.000s elapsed)
  PROF    time    0.048s  (  0.048s elapsed)
  EXIT    time    0.000s  (  0.000s elapsed)
  Total   time    7.134s  (  7.140s elapsed)

  Alloc rate    1,991,176,794 bytes per MUT second

  Productivity  76.1% of total user, 76.1% of total elapsed
</code></pre>
<p>Awesome! We got the memory usage down to a total of 12 MB, we shaved off another second from the total runtime and the productivity is also up a notch. Lets take a look at the graphs to see where we stand:</p>
<div class="figure">
<img src="../images/2016-09-22-HaskellProfiling-OptimizedVersion-hc.jpg" title="memory usage using -hc" alt="memory usage using -hc" />
<p class="caption">memory usage using -hc</p>
</div>
<div class="figure">
<img src="../images/2016-09-22-HaskellProfiling-OptimizedVersion-hy.jpg" title="memory usage using -hy" alt="memory usage using -hy" />
<p class="caption">memory usage using -hy</p>
</div>
<p>This looks a lot better. Ratings occupy most of the space, and they should. We do infact read the initial ratings and then hold on to them for the entire run of the program. At every dimension, we also maintain residuals along with the ratings. For all dimensions, we keep appending to the user and item vectors, so it makes sense that the memory usage by Maps would grow a little over the run of the program.</p>
<p>There is one last thing that I want to show before we end this discussion. Note that we were always using the <em>profiling</em> executable when generating the graphs. Haskell compilation can perform a large number of whole-sale optimizations that exploit the pure nature of the code. When generating profiling binaries, though, the compiler holds back some optimizations. This can be seen clearly, when we run the optimized binary.</p>
<pre><code>   6,999,678,952 bytes allocated in the heap
   1,252,786,760 bytes copied during GC
       2,321,296 bytes maximum residency (207 sample(s))
         136,280 bytes maximum slop
               7 MB total memory in use (0 MB lost due to fragmentation)

                                     Tot time (elapsed)  Avg pause  Max pause
  Gen  0     13041 colls,     0 par    0.879s   0.881s     0.0001s    0.0004s
  Gen  1       207 colls,     0 par    0.217s   0.219s     0.0011s    0.0019s

  TASKS: 4 (1 bound, 3 peak workers (3 total), using -N1)

  SPARKS: 0 (0 converted, 0 overflowed, 0 dud, 0 GC'd, 0 fizzled)

  INIT    time    0.000s  (  0.000s elapsed)
  MUT     time    3.136s  (  3.149s elapsed)
  GC      time    1.096s  (  1.099s elapsed)
  EXIT    time    0.000s  (  0.000s elapsed)
  Total   time    4.262s  (  4.249s elapsed)

  Alloc rate    2,232,027,670 bytes per MUT second

  Productivity  74.3% of total user, 74.5% of total elapsed
</code></pre>
<p>The optimized run shaved off nearly 50% of both the total run-time and total memory usage.</p>
<p><em>All in all, we went from consuming around 650Mb and taking about 20s to using just 12Mb and taking only 7s. That is a nearly 50x reduction in memory usage and a 3x speed-up.</em></p>
<h2 id="conclusions">Conclusions</h2>
<p>Haskell is inherently a pure functional lazy language. This laziness helps optimize away things that are never needed and can help in a lot of patterns like infinite streams. The pure functionalness lets GHC do a lot of optimizations on the code. However, when using it for computationally intensive tasks like here, where we know we would be evaluating all the numbers and every iteration would depend on the result of the previous iteration, giving the compiler indications using the <em>Bang Patterns</em> and/or <em>seq</em> can go a long way in getting significant speed-ups and memory efficiency.</p>
<p>The profiling binaries can give a very good idea of what the program run-time memory usage looks like. We can also generate charts to help guide the optimization effort. Once, all that is done, we should expect the optimized binaries to peform a lot better than the profiling ones even.</p>
<p>We only scratched the surface with memory profiling. GHC offers a host of other tools for time and allocation profiling, as documented <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html">here</a>.</p>
<h2 id="reproducing-these-stats-and-charts">Reproducing these stats and charts</h2>
<p>The code for both the initial and optimized version is available <a href="https://github.com/shubhamchopra/RecommenderSystem">here</a>. To get movie recommendations, you can get data from Netflix or <a href="http://grouplens.org/datasets/movielens/">MovieLens</a>. I am currently using GHC 7.10.3. You might get different numbers depending on the sample of data you use, the processor and such, but the shape/pattern of the charts would likely be similar.</p>
</section>
<section>
<div id="disqus_thread"></div>
<script>
/**
  * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
  */

   var disqus_config = function () {
   this.page.url = window.location.href.split('/').splice(0,3).join("/")+'/posts/2016-09-22-Haskell-Profiling-Funk-SVD-case-study.html'; // Replace PAGE_URL with your page's canonical URL variable
   this.page.identifier = '/posts/2016-09-22-Haskell-Profiling-Funk-SVD-case-study.html'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
   };
   
(function() { // DON'T EDIT BELOW THIS LINE
 var d = document, s = d.createElement('script');

 s.src = '//blog-shubhamchopra-com.disqus.com/embed.js';

 s.setAttribute('data-timestamp', +new Date());
 (d.head || d.body).appendChild(s);
 })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</section>

        </div>
        <div id="footer">
          Site proudly generated by
          <a href="http://jaspervdj.be/hakyll">Hakyll</a>.             This theme was designed by <a href="http://twitter.com/katychuang">Dr. Kat</a> and showcased in the <a href="http://katychuang.com/hakyll-cssgarden/gallery/">Hakyll-CSSGarden</a>

        </div>
    </body>
</html>
